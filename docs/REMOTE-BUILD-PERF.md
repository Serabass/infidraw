# Сборка через удалённый Docker: почему долго

При `DOCKER_HOST=tcp://...` (другой хост в сети) сборка тормозит по двум причинам.

## 1. Контекст гоняется по сети для каждого таргета

**Как устроено:** клиент (твоя машина) не собирает образ — он отправляет **build context** (файлы из `context = "./services/event-store"` и т.п.) на удалённый демон. Buildx bake запускает несколько таргетов, и для **каждого** таргета контекст передаётся заново.

- 8 таргетов → до 8 передач контекста по сети.
- Чем больше размер контекста (с учётом `.dockerignore`), тем дольше.
- На 100 Mbit/s десятки мегабайт — это уже секунды на таргет; на 10 Mbit/s — десятки секунд.

Поэтому «долго уже на первом шаге» часто означает: долго шла **передача контекста**, а потом уже пошёл первый шаг (например, `apk`).

**Что сделано:** во всех сервисах и во frontend ужесточён `.dockerignore` (исключены `node_modules`, `dist`, `.git`, `*.md`, e2e, лишние конфиги), чтобы не тащить лишнее по сети.

**Что можно ещё:**  
- Собирать с той же машины, где крутится Docker (SSH на хост и запуск `bake` там) — тогда контекст не идёт по сети.  
- Проверить размер контекста: `docker buildx build --progress=plain -f <Dockerfile> . 2>&1` и смотреть фазу «transferring context».

## 2. RUN (apk, npm и т.д.) выполняются на удалённой машине

Сам шаг `RUN apk add ...` или `RUN npm ci` выполняется **на удалённом хосте**, а не у тебя в ноутбуке.

- Если у этого хоста медленный или нестабильный интернет — медленно качаются пакеты Alpine (apk) или npm-пакеты.
- Если у хоста слабый CPU или мало RAM — шаги с компиляцией/установкой тоже будут долгими.

То есть «даже apk долго» может быть из-за сети/ресурсов **удалённой машины**, а не из-за твоей локальной сети до неё.

**Что можно:**  
- На удалённом хосте проверить скорость до зеркал Alpine и до registry.npmjs.org.  
- При необходимости сменить зеркало Alpine в Dockerfile (`sed` в `/etc/apk/repositories` на быстрый mirror).

## Итог

| Причина              | Где проявляется              | Что делать                          |
|----------------------|-----------------------------|-------------------------------------|
| Передача контекста   | Перед первым шагом таргета  | Жёсткий `.dockerignore`, меньше контекст |
| Сеть/CPU удалённого  | Во время RUN (apk, npm)     | Быстрые зеркала, сборка с того же хоста |

После первого успешного билда кеш в registry (`:buildcache`) ускоряет повторные сборки: меньше шагов пересчитывается, но **передача контекста** по-прежнему происходит при каждой сборке.

---

## Как ещё ускорить

### 1. Собирать только изменённое: `-ChangedOnly`

Вместо восьми таргетов — только те, в чьих директориях есть изменения относительно `origin/main` (или `HEAD~1`, если ветки нет):

```powershell
.\bake.ps1 -ChangedOnly
```

Меньше передач контекста и меньше образов — самый быстрый вариант при правках в 1–2 сервисах.

### 2. Сборка на том же хосте, что и Docker

Контекст не идёт по сети: репо на удалённой машине, bake запускается там (по SSH).

- Клонировать/подтянуть репо на хост (192.168.88.13).
- На хосте: `export DOCKER_HOST=unix:///var/run/docker.sock` (или не ставить, если демон локальный).
- Запустить `./bake.ps1` (или через PowerShell Core).
- Образы пушатся в registry с этого же хоста.

Так убирается главный тормоз — передача контекста по сети.

### 3. Быстрое зеркало Alpine в Dockerfile

Если на удалённом хосте медленно тянется apk, в начале Dockerfile (после `FROM`) можно переключить репозиторий:

```dockerfile
FROM node:20-alpine AS deps
RUN sed -i 's|http://dl-cdn.alpinelinux.org|https://mirror.yandex.ru/mirrors/alpine|g' /etc/apk/repositories \
  && apk add --no-cache ...
```

Или другой быстрый mirror (например, по региону).

### 4. Меньше параллелизма при узком канале

Bake по умолчанию гоняет несколько таргетов параллельно — несколько контекстов по сети одновременно. Если канал узкий, можно собирать по одному таргету, чтобы каждый контекст шёл на полной скорости:

```powershell
.\bake.ps1 -FrontendOnly
# или по одному сервису в цикле при необходимости
```

`-ChangedOnly` уже сокращает число таргетов и тем самым снижает конкуренцию за канал.

### 5. Ресурсы удалённого хоста

- Достаточно RAM для BuildKit (иначе своп и тормоза).
- CPU: сборка Node/Angular любит ядра; при нехватке шаги типа `npm run build` будут долгими.

---

## Ошибка TLS при cache-from / cache-to (registry)

Если при bake появляется:

```text
failed to configure registry cache importer: ... Head "https://reg.serabass.kz/...": tls: failed to verify certificate: x509: certificate signed by unknown authority
```

значит BuildKit (в buildx builder) не доверяет сертификату реестра `reg.serabass.kz` (самоподписанный или частный CA). У buildx с драйвером `docker-container` настройки реестра берутся из **конфига BuildKit**, а не из `daemon.json`.

**Что сделано в репозитории:** в корне лежит `buildkitd.toml` с `[registry."reg.serabass.kz"] insecure = true`. Скрипт `bake.ps1` при создании builder'а передаёт этот конфиг в `docker buildx create --buildkitd-config buildkitd.toml`. Если builder уже был создан **без** этого конфига (или ошибка TLS остаётся), пересоздай builder **одним** из способов:

```powershell
# Вариант 1: флаг (рекомендуется)
.\bake.ps1 -RecreateBuilder

# Вариант 2: вручную
$env:DOCKER_HOST = "tcp://192.168.88.13:32375"
docker buildx rm infidraw-remote
.\bake.ps1
```

При следующем запуске bake builder создастся заново с конфигом из `buildkitd.toml`, и cache-from/cache-to в `reg.serabass.kz` будут без проверки TLS.

**Альтернатива (без buildkitd.toml):** на удалённом хосте (192.168.88.13) в `/etc/docker/daemon.json` добавить `"insecure-registries": ["reg.serabass.kz"]` и перезапустить Docker. Для классического `docker pull` этого достаточно; для buildx с docker-container драйвером конфиг BuildKit (как выше) надёжнее.
